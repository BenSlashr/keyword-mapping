# Keyword-URL Matcher v2 Premium

> Outil d'assignation automatique de mots-cl√©s vers des pages web avec scoring hybride avanc√©

## üöÄ Fonctionnalit√©s

- **Assignation automatique** de mots-cl√©s vers pages avec algorithme de scoring hybride
- **Embeddings passage-level** avec FAISS pour recherche s√©mantique ultra-rapide
- **Scoring hybride** combinant embeddings, BM25, similarit√© de titre et num√©rique
- **Seuil adaptatif** bas√© sur Q3 - 1.5*IQR pour optimisation automatique
- **D√©tection de cannibalisation** via int√©gration Google Search Console
- **Interface web moderne** avec Alpine.js et Tailwind CSS
- **Exports multiples** : Excel format√©, CSV, JSON, rapport HTML
- **Monitoring temps r√©el** avec m√©triques Prometheus
- **Crawling en direct** et support sitemap XML
- **API RESTful** compl√®te avec WebSockets

## üìã Pr√©requis

- Python 3.9+
- Redis Server
- 8GB RAM minimum (16GB recommand√©)
- GPU optionnel (acc√©l√©ration FAISS)

## üõ†Ô∏è Installation

### 1. Cloner le repository
```bash
git clone https://github.com/votre-org/keyword-url-matcher-v2.git
cd keyword-url-matcher-v2
```

### 2. Cr√©er un environnement virtuel
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

### 3. Installer les d√©pendances
```bash
pip install -r requirements.txt
```

### 4. Configuration
```bash
cp env.example .env
# √âditer .env avec vos param√®tres
```

Variables d'environnement importantes :
```env
REDIS_URL=redis://localhost:6379/0
GOOGLE_CLIENT_ID=your_google_client_id
GOOGLE_CLIENT_SECRET=your_google_client_secret
MAX_KEYWORDS=1000000
MAX_PAGES=50000
```

### 5. D√©marrer Redis
```bash
redis-server
```

### 6. D√©marrer l'application
```bash
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

L'application sera accessible sur `http://localhost:8000`

## üìä Utilisation

### Interface Web

1. **Upload des mots-cl√©s** : Fichier CSV avec colonnes `keyword, volume`
2. **Source des pages** : 
   - CSV avec colonnes `url, content`
   - URL de sitemap XML
   - Crawl en direct depuis une URL seed
3. **Configuration avanc√©e** :
   - Seuil minimal de score
   - Pond√©ration du volume
   - Nombre de suggestions alternatives
   - Activation Search Console
4. **Monitoring en temps r√©el** via WebSocket
5. **Export des r√©sultats** en multiple formats

### API REST

#### Cr√©er un job de matching
```bash
curl -X POST "http://localhost:8000/jobs/match" \
  -F "keywords_file=@keywords.csv" \
  -F "pages_file=@pages.csv" \
  -F "source_type=csv" \
  -F "min_score_threshold=0.50"
```

#### Suivre la progression
```bash
curl "http://localhost:8000/jobs/{job_id}"
```

#### R√©cup√©rer les r√©sultats
```bash
# JSON
curl "http://localhost:8000/jobs/{job_id}/result"

# Excel
curl "http://localhost:8000/jobs/{job_id}/result?format=xlsx" -o results.xlsx

# CSV
curl "http://localhost:8000/jobs/{job_id}/result?format=csv" -o results.csv
```

## üß† Algorithme de Scoring

### Composants du score hybride

```python
score = 0.55 * embedding_sim + 0.25 * bm25_score + 0.10 * title_sim + 0.10 * numeric_sim
```

1. **Embedding Similarity (55%)** : Similarit√© cosinus entre embeddings
2. **BM25 Score (25%)** : Score BM25 traditionnel
3. **Title Similarity (10%)** : Similarit√© avec le titre de page
4. **Numeric Similarity (10%)** : Correspondance des valeurs num√©riques

### Seuil adaptatif
```python
score_threshold = max(Q3(scores) - 1.5 * IQR(scores), 0.50)
```

## üìà Performance

### Benchmarks

| Taille | Temps (8 vCPU) | RAM | 
|--------|----------------|-----|
| 50k KW √ó 5k pages | ‚â§ 6 min | < 8 GB |
| 1M KW √ó 50k pages | ‚â§ 40 min | 32 GB |

### Optimisations

- **FAISS HNSW** : Index haute performance avec ef=200, M=32
- **Chunking intelligent** : 512 tokens avec overlap 128
- **Cache embeddings** : SHA-256 based, √©vite les recalculs
- **Processing parall√®le** : Traitement par batches optimis√©

## üîç Monitoring

### M√©triques Prometheus

- `keyword_matcher_keywords_processed_total` : Mots-cl√©s trait√©s
- `keyword_matcher_processing_duration_seconds` : Temps de traitement
- `keyword_matcher_faiss_queries_total` : Requ√™tes FAISS
- `keyword_matcher_memory_usage_bytes` : Utilisation m√©moire
- `keyword_matcher_active_jobs` : Jobs actifs

### Endpoints de sant√©

```bash
# Status g√©n√©ral
curl http://localhost:8000/health

# M√©triques d√©taill√©es
curl http://localhost:8000/metrics
```

## üîß Configuration avanc√©e

### Weights du scoring
```python
WEIGHTS = {
    "embedding": 0.55,  # Poids des embeddings
    "bm25": 0.25,       # Poids BM25
    "title": 0.10,      # Poids titre
    "numeric": 0.10     # Poids num√©rique
}
```

### Param√®tres FAISS
```python
FAISS_EF_SEARCH = 200      # Pr√©cision recherche
FAISS_M_CONNECTIONS = 32   # Connections HNSW
```

### Chunking
```python
CHUNK_SIZE = 512          # Taille des chunks
CHUNK_OVERLAP = 128       # Overlap entre chunks
```

## üîê Int√©gration Google Search Console

### Configuration OAuth

1. Cr√©er un projet dans Google Cloud Console
2. Activer l'API Search Console
3. Cr√©er des identifiants OAuth 2.0
4. Configurer les variables d'environnement

### D√©tection de cannibalisation

L'outil compare automatiquement :
- URLs assign√©es par l'algorithme
- URLs top CTR dans Search Console (90 derniers jours)
- Calcul de perte de confiance estim√©e

## üìä Formats d'export

### Excel (.xlsx)
- **Summary** : Statistiques g√©n√©rales
- **Assignments** : Assignations d√©taill√©es
- **Orphans** : Mots-cl√©s orphelins
- **Cannibalization** : Alertes de cannibalisation

### CSV
- Format unifi√© avec colonnes `type` pour filtrage
- Encoding UTF-8 avec BOM

### JSON
- Structure compl√®te des r√©sultats
- M√©tadonn√©es d'export incluses

### HTML
- Rapport interactif avec graphiques
- Top assignations et distributions

## üê≥ D√©ploiement Docker

```bash
# Build
docker build -t keyword-matcher-v2 .

# Run
docker run -d \
  --name keyword-matcher \
  -p 8000:8000 \
  -e REDIS_URL=redis://redis:6379/0 \
  keyword-matcher-v2
```

### Docker Compose
```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

## üß™ Tests

```bash
# Tests unitaires
pytest tests/

# Tests d'int√©gration
pytest tests/integration/

# Tests de performance
pytest tests/performance/ -v
```

### Golden Set
- 200 mots-cl√©s labellis√©s manuellement
- Precision@1 ‚â• 0.85 requis en CI
- R√©gression automatique d√©tect√©e

## üìù API Documentation

Documentation interactive disponible sur :
- Swagger UI : `http://localhost:8000/docs`
- ReDoc : `http://localhost:8000/redoc`

### Endpoints principaux

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/jobs/match` | Cr√©er un job de matching |
| GET | `/jobs/{id}` | Status du job |
| GET | `/jobs/{id}/result` | R√©sultats du job |
| DELETE | `/jobs/{id}` | Annuler un job |
| GET | `/health` | Sant√© de l'application |
| GET | `/metrics` | M√©triques Prometheus |

## ü§ù Contribution

1. Fork le repository
2. Cr√©er une branche feature : `git checkout -b feature/amazing-feature`
3. Commit : `git commit -m 'Add amazing feature'`
4. Push : `git push origin feature/amazing-feature`
5. Ouvrir une Pull Request

## üìÑ Licence

Ce projet est sous licence MIT. Voir `LICENSE` pour plus de d√©tails.

## üè¢ Support

D√©velopp√© par **SLASHK** - Agence SEO √† Lille

- üìß Email : contact@slashk.fr
- üåê Website : https://slashk.fr
- üì± LinkedIn : @slashk-seo

## üîÑ Changelog

### v2.0.0 (2024)
- ‚ú® Algorithme de scoring hybride
- ‚ú® Interface web moderne
- ‚ú® Int√©gration Search Console
- ‚ú® Monitoring Prometheus
- ‚ú® Exports multiples
- ‚ú® API WebSocket temps r√©el

### v1.0.0 (2023)
- üéâ Version initiale CLI
- üìä Scoring basique TF-IDF 